# Estudo Comparativo de Ferramentas de ProgramaÃ§Ã£o Reativa

**âš ï¸ AtenÃ§Ã£o: Este projeto estÃ¡ em andamento (Work in Progress).**

## Sobre o Projeto

Este repositÃ³rio contÃ©m o cÃ³digo-fonte e os materiais de apoio para um estudo prÃ¡tico e comparativo entre diferentes ferramentas e linguagens de programaÃ§Ã£o reativa.

O objetivo Ã© analisar e comparar o desempenho, o uso de recursos (CPU e memÃ³ria) e a complexidade de desenvolvimento de microserviÃ§os equivalentes, implementados com cada uma das tecnologias listadas abaixo. Conforme descrito no artigo de referÃªncia, cada serviÃ§o Ã© submetido a testes de carga para avaliar mÃ©tricas como throughput e latÃªncia sob alta concorrÃªncia.

## Tecnologias Analisadas

- **Java**: Project Reactor (Spring WebFlux)
- **Kotlin**: Coroutines + Flow (Spring WebFlux)
- **Go**: RxGo
- **Python**: RxPY

## Ambiente de ExecuÃ§Ã£o

Os experimentos e testes de carga sÃ£o executados localmente com Docker Compose (para desenvolvimento) e na Google Cloud Platform (GCP), utilizando o **Cloud Run** para hospedar cada microserviÃ§o de forma independente e escalÃ¡vel.

> **Nota:** As instruÃ§Ãµes completas de build/deploy serÃ£o finalizadas ao tÃ©rmino do estudo (WIP).

## Estrutura do RepositÃ³rio (WIP)

- InstruÃ§Ãµes para build e execuÃ§Ã£o local via Docker.
- Roteiro para implantaÃ§Ã£o no Cloud Run.
- Scripts para geraÃ§Ã£o de carga e coleta de mÃ©tricas.

---

## âœ… Regras de Paridade (Optimization Parity Checklist)

Para garantir um comparativo **justo** entre as linguagens/tecnologias, adotamos um conjunto de regras transversais. **Qualquer otimizaÃ§Ã£o aplicada a uma tecnologia precisa ter equivalente nas demais**.

### 1) Contrato e SemÃ¢ntica do Pipeline

- [ ] Mesmo **pipeline lÃ³gico** em todos os serviÃ§os: `range â†’ map â†’ filter â†’ batch (lotes) â†’ chamadas HTTP externas â†’ reduce/fold (mÃ©tricas)`.
- [ ] **Batch real** em todas as linguagens:
  - Reactor: `buffer(batch)` â†’ listas de tamanho `batch`;
  - Kotlin Flow: operador `chunkedFlow(batch)` customizado (nÃ£o confundir com `buffer(n)` de canal);
  - Go/Python: agregaÃ§Ã£o explÃ­cita antes de disparar chamadas.
- [ ] **Sem desserializar o corpo** do downstream quando nÃ£o Ã© usado:
  - Validar apenas `status 2xx` e **descartar** o corpo (ex.: `exchangeToMono(...).thenReturn(status)` / `awaitExchange { ...; releaseBody() }` / `io.Copy(io.Discard, resp.Body)` / `resp.release()`).
- [ ] **Retries equivalentes**:
  - Mesma contagem por falha (`RETRY_ATTEMPTS`) e **mesmo backoff** (ex.: exponencial com limite de 300 ms);
  - Mesma polÃ­tica para timeout/5xx.

### 2) Cliente HTTP e Pool de ConexÃµes

- [ ] **Keep-alive** habilitado e **limite de conexÃµes por host** equivalente.
- [ ] **Timeouts equivalentes** em todas as stacks:
  - `CONNECT_TIMEOUT`, `RESPONSE/READ_TIMEOUT` e `WRITE_TIMEOUT`;
  - Em Netty (WebFlux): `ReadTimeoutHandler` e `WriteTimeoutHandler` adicionados no pipeline.
- [ ] **Descartar corpo** para reduzir overhead (ver item anterior).
- [ ] CabeÃ§alhos e comportamento idÃªnticos (ex.: sem compressÃ£o seletiva em apenas uma linguagem).

### 3) ConcorrÃªncia e ExecuÃ§Ã£o

- [ ] **Controle de concorrÃªncia em dois nÃ­veis (quando pertinente)**:
  - **Por lote** (quantos lotes avanÃ§am simultaneamente) e **por item** (quantas chamadas simultÃ¢neas por lote);
  - VariÃ¡veis padronizadas:
    - `BATCH_CONCURRENCY` (default: 4)
    - `ITEM_CONCURRENCY` (default: 64)
- [ ] **Limite global de inflight** equivalente entre serviÃ§os.
- [ ] **Trabalho de CPU leve** (map/filter) **fora do event-loop**:
  - Kotlin: `flowOn(Dispatchers.Default)` para etapas CPU leves;
  - Reactor: manter no event-loop; sÃ³ usar `boundedElastic` se simular I/O bloqueante local.
- [ ] Evitar **materializar coleÃ§Ãµes grandes**:
  - Preferir `reduce/fold` para contadores em vez de `collectList/toList`.

### 4) Ambiente e Recursos

- [ ] **Mesmos limites** de CPU e memÃ³ria por contÃªiner (ex.: 1 vCPU, 512 MiB).
- [ ] **Mesmo conjunto de CPUs** (cpuset) quando local, para isolar ruÃ­do.
- [ ] **Warm-up** padronizado (ex.: 30â€“60 s) antes da mediÃ§Ã£o para estabilizar JIT/GC/loop.
- [ ] **NÃ­vel de logs mÃ­nimo** durante os testes (apenas resumo por requisiÃ§Ã£o/etapa).

### 5) Carga e Metodologia

- [ ] Workload **fechado por taxa (RPS)** alÃ©m de testes por VUs, para mitigar â€œcoordinated omissionâ€.
- [ ] **CenÃ¡rios base** padronizados:
  - `io_delay_ms âˆˆ {10, 50, 200}` (baixa, mÃ©dia e alta latÃªncia do serviÃ§o externo);
  - `batch âˆˆ {50, 100, 500}`;
  - Rampas de taxa/concorrÃªncia idÃªnticas entre serviÃ§os.
- [ ] **Jitter** controlado no serviÃ§o externo (`slow-io`) para variaÃ§Ã£o realista e comparÃ¡vel.

### 6) MÃ©tricas e Coleta

- [ ] MÃ©tricas por cenÃ¡rio:
  - **Throughput efetivo** (req/s e eventos/s);
  - **LatÃªncias** p50/p95/p99;
  - **Erros** (timeouts/5xx);
  - **CPU% e RSS** mÃ©dios/mÃ¡ximos por serviÃ§o.
- [ ] **2â€“3 repetiÃ§Ãµes** por cenÃ¡rio; reportar **mÃ©dia** e **desvio**.
- [ ] Outputs dos clientes de carga e logs dos serviÃ§os salvos em arquivos (JSON/CSV/Parquet) para reprodutibilidade.

---

## ğŸ”§ ParÃ¢metros e VariÃ¡veis de Ambiente (padrÃ£o)

> Cada serviÃ§o expÃµe variÃ¡veis equivalentes. Ajuste por cenÃ¡rio, mantendo paridade.

| VariÃ¡vel | DescriÃ§Ã£o | Default |
|---|---|---|
| `PORT` | Porta do serviÃ§o | por projeto |
| `MAX_COUNT` | Limite de itens por requisiÃ§Ã£o | `200000` |
| `BATCH_CONCURRENCY` | Lotes processados em paralelo | `4` |
| `ITEM_CONCURRENCY` | Itens (chamadas externas) por lote em paralelo | `64` |
| `DOWNSTREAM_TIMEOUT_MS` | Timeout por requisiÃ§Ã£o ao serviÃ§o externo | `2000` |
| `RETRY_ATTEMPTS` | Tentativas adicionais em falha | `1` |
| `LOG_LEVEL` | NÃ­vel de log | `INFO` |

**Payload padrÃ£o (`POST /process`):**
```json
{
  "count": 10000,
  "batch": 100,
  "io_delay_ms": 50,
  "downstream_url": "http://slow-io:8080/slow"
}
```

---

## ğŸ§ª CenÃ¡rios de Teste Sugeridos

1. **CenÃ¡rio Principal (baseline)**
   - `io_delay_ms=50`, `batch=100`
   - Rampas de taxa: 200 â†’ 400 â†’ 800 rps (2â€“3 min por degrau)
   - Warm-up: 60 s

2. **VariaÃ§Ã£o de LatÃªncia**
   - `io_delay_ms âˆˆ {10, 200}`
   - Demais parÃ¢metros iguais ao baseline

3. **VariaÃ§Ã£o de Batch**
   - `batch âˆˆ {50, 500}`
   - `io_delay_ms=50`

> Em todos os cenÃ¡rios: repetir 2â€“3 vezes, coletar p50/p95/p99, throughput, erros, CPU, RSS.

---

## ğŸ” Reprodutibilidade

- **VersÃµes congeladas** no apÃªndice (WIP): JDK/Temurin, Kotlin, Spring Boot, Reactor, RxGo, Go, Python, RxPY, aiohttp, k6/wrk.
- **Dockerfiles** e **compose** com limites declarados de CPU/memÃ³ria.
- **Scripts** de carga com parÃ¢metros via ENV (WIP) e export de resultados em JSON.
- **ETL** simples (WIP) para consolidar resultados em CSV/Parquet e gerar grÃ¡ficos.

---

## âš ï¸ ObservaÃ§Ãµes Importantes

- As otimizaÃ§Ãµes implementadas seguem o **Optimization Parity Checklist**. Caso alguma tecnologia receba uma otimizaÃ§Ã£o **sem equivalente** nas demais, essa alteraÃ§Ã£o deve ser:
  1) Revertida, **ou**
  2) Replicada nas outras stacks, **ou**
  3) Explicitamente documentada como **fora do budget** e **excluÃ­da** da comparaÃ§Ã£o principal.

- O **serviÃ§o `slow-io`** Ã© a Ãºnica dependÃªncia externa usada para simular I/O bloqueante com latÃªncia controlada. O teste busca comparar **o manejo de espera/concorrÃªncia** de cada stack, e nÃ£o a eficiÃªncia de parsing/serializaÃ§Ã£o de payloads.

---

## Roadmap (WIP)

- [ ] Consolidar scripts de carga (k6/wrk) e ETL de mÃ©tricas.
- [ ] Publicar guias de execuÃ§Ã£o local e no Cloud Run.
- [ ] Incluir grÃ¡ficos comparativos e relatÃ³rio final.
- [ ] Adicionar apÃªndice de versÃµes/flags/commits usados.

---